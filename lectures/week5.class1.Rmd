---
title: 'Statistical Methods for Insurance: Linear Models'
author: Di Cook & Souhaib Ben Taieb, Econometrics and Business Statistics, Monash
  University
date: "W5.C1"
output:
  ioslides_presentation:
    transition: default
    widescreen: yes
  beamer_presentation: default
css: default.css
---

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  echo=FALSE,
  fig.height = 2,
  fig.width = 5,
  collapse = TRUE,
  comment = "#>"
)
options(digits=2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(gridExtra)
library(xkcd)
```

## Overview of this class

- Quiz 3
- Linear model diagnostics
- Transformations
- READING: Ch 6, Diez, Barr, Cetinkaya-Rundel

## Modeling Olympic medal counts

We fit the medal count for 2012, purely on the counts from 2008, to illustrate the influence diagnostics.

```{r}
olympics2008 <- read_csv("../data/olympics2008.csv")
olympics2012 <- read_csv("../data/olympics2012.csv")

oly <- merge(olympics2012[,c("Country","Total")],
             olympics2008[,c("Country","Total")], by="Country",
             all.x=TRUE)
colnames(oly)[2] <- "M2012"
colnames(oly)[3] <- "M2008"
oly <- oly %>% filter(!is.na(M2008))

oly_lm <- glm(M2012~M2008, data=oly)
library(broom)
kable(tidy(oly_lm))
coefs <- tidy(oly_lm)
```

Giving the model, 

$M_{2012}=$ `r coefs$estimate[1]` $+$ `r coefs$estimate[2]` $M_{2008} + \varepsilon$

## Your turn

- Should the model be re-fit with the intercept forced to ZERO?

## 

```{r fig.align='center', fig.width==6, fig.height=4}
ggplot(oly, aes(x=M2008, y=M2012)) + geom_point() +
  geom_smooth(method="lm", se=FALSE) + theme_xkcd() + xkcdaxis(c(0,110),c(0,110))
```

## Model diagnostics

- Based on `leave-one-out` statistics
- For $n$ observations, fit $n$ models where each model has one observation removed. 
- Let's take a look at fitting the medal tallies, without the USA.

```{r}
oly_noUSA <- oly %>% filter(Country != "UnitedStates")
oly_noUSA_lm <- glm(M2012~M2008, data=oly_noUSA)
coefs_noUSA <- tidy(oly_noUSA_lm)
comp_estimates <- data.frame(all=coefs$estimate,
                             noUSA=coefs_noUSA$estimate)
comp_estimates$estimate <- c("intercept", "slope")
kable(comp_estimates, format = "markdown", row.names=TRUE)
```

-
- Parameter estimates change a little

##

```{r fig.align='center', fig.width=6, fig.height=4}
comp_estimates_m <- comp_estimates %>% 
  gather(model, stat, -estimate) %>%
  spread(estimate, stat)
ggplot(oly, aes(x=M2008, y=M2012)) + geom_point() +
  geom_abline(data=comp_estimates_m, 
              aes(intercept=intercept, slope=slope, colour=model))  + theme_xkcd() + xkcdaxis(c(0,110),c(0,110))
```

## Other model fit parameters

- deviance
- predicted values, residuals

```{r}
comp_diag <- data.frame(
  null.dev=c(glance(oly_lm)$null.deviance, glance(oly_noUSA_lm)$null.deviance),
  deviance=c(glance(oly_lm)$deviance, glance(oly_noUSA_lm)$deviance), 
  fitted=c(predict(oly_lm, oly[oly$Country=="UnitedStates",]), 
           predict(oly_noUSA_lm, oly[oly$Country=="UnitedStates",])), 
  resid=c(oly$M2012[oly$Country=="UnitedStates"]-
            predict(oly_lm, oly[oly$Country=="UnitedStates",]),
          oly$M2012[oly$Country=="UnitedStates"]-
            predict(oly_noUSA_lm, oly[oly$Country=="UnitedStates",])))
rownames(comp_diag) <- c("All", "No USA")
kable(comp_diag, format="markdown", row.names=TRUE)
```

## What it could look like

```{r fig.align='center', fig.width=6, fig.height=4}
set.seed(2468)
df <- data.frame(x=c(runif(99), 10))
df$y <- df$x+c(rnorm(99), 6)
df_all <- glm(y~x, data=df)
df_no <- glm(y~x, data=df[-100,])
df_est <- data.frame(all=tidy(df_all)$estimate,
                             no=tidy(df_no)$estimate)
df_est$estimate <- c("intercept", "slope")
df_est_m <- df_est %>% 
  gather(model, stat, -estimate) %>%
  spread(estimate, stat)
ggplot(df, aes(x=x, y=y)) + geom_point() +
  geom_abline(data=df_est_m, 
              aes(intercept=intercept, slope=slope, colour=model))  + theme_xkcd() + xkcdaxis(c(0,10),c(0,16))
```

## Leverage

Leverage $h_{ii}$ is defined for each observation, $1, ..., n$, and is the $i^{th}$ diagonal element of the hat matrix:

$$H=X(X^TX)^{-1}X^T$$

where $X$ is the design matrix, e.g. for $\beta_0+\beta_1x$, 

$$X=\left[ \begin{array}{cc} 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n \end{array} \right]$$

Intuitively, observations which are far from the mean of the explanatory variables will have higher leverage. 

YOU CAN CALCULATE THIS WITHOUT FITTING ALL n MODELS!

## Highest leverage for medal tally model

```{r}
oly_diag <- augment(oly_lm)
oly_diag$Country <- oly$Country
oly_diag %>% arrange(desc(.hat)) %>% select(Country, .hat) %>% head(15)
```

Cutoff for high leverage is $2p/n = 2*1/73 = 0.027$.

## Plot of leverage

```{r fig.align='center', fig.width=6, fig.height=4}
ggplot(oly_diag, aes(x=.hat)) + geom_histogram() +
  geom_vline(xintercept=0.027, colour="red") + 
  theme_xkcd() + xkcdaxis(c(0,0.33),c(0,40))
```

## Log-tranform the counts

```{r fig.align='center', fig.width=6, fig.height=4}
oly_tf <- oly
oly_tf$M2008 <- log10(oly_tf$M2008)
oly_tf$M2012 <- log10(oly_tf$M2012)
ggplot(oly_tf, aes(x=M2008, y=M2012)) + geom_point() + 
  xlab("Log counts 2008") + ylab("Log counts 2012") + 
  theme_xkcd() + xkcdaxis(c(0,2.2),c(0,2.2))
```

##

```{r}
oly_tf_lm <- glm(M2012~M2008, data=oly_tf)
oly_tf_diag <- augment(oly_tf_lm)
oly_tf_diag$Country <- oly$Country
oly_tf_diag %>% arrange(desc(.hat)) %>% select(Country, .hat) %>% head(15)
```

##

```{r fig.align='center', fig.width=6, fig.height=4}
ggplot(oly_tf_diag, aes(x=.hat)) + geom_histogram() +
  geom_vline(xintercept=0.027, colour="red") + 
  theme_xkcd() + xkcdaxis(c(0,.10),c(0,25))
```

Transforming skewed variables reduces the influence of any one, or few points. The distribution is more even, and the highest leverage value is much lower now.

## Hat values for simulated data

```{r fig.align='center', fig.width=6, fig.height=4}
df_lm <- glm(y~x, data=df)
df_diag <- augment(df_lm)
ggplot(df_diag, aes(x=.hat)) + geom_histogram() +
  geom_vline(xintercept=0.02, colour="red") + 
  theme_xkcd() + xkcdaxis(c(0,1),c(0,100))
```

## Cooks D

Leverage takes no notice of the response variable. So the USA did not have a huge influence because its medal count in 2012 was similar to that in 2008, so it was close to the trend. If for some reason the medal count in 2012 was 0, the line with the USA would be much more drawn away from the other countries. 

Cooks D, and DFFITS, also use the response variable, to assess influence. 

$$D_i = \frac{e_i^2}{{MSE}^2p}\frac{h_{ii}}{(1-h_{ii})^2}$$

where $e_i$ is the $i^{th}$ residual, $p=$number of explanatory variables, and MSE is the mean squared error of the linear model.

Values greater than $4/n$ are large, by a rule of thumb. Or alternatively, greater than 1 is another rule of thumb.

## Cooks D for Olympic medal tally

$$ Raw ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Transformed $$

```{r}
tbl1 <- oly_diag %>% arrange(desc(.cooksd)) %>% select(Country, .cooksd) 
tbl2 <- oly_tf_diag %>% arrange(desc(.cooksd)) %>% select(Country, .cooksd)
tbl <- cbind(tbl1, tbl2)
kable(tbl)
```

##

```{r fig.align='center', fig.width=10, fig.height=4}
p1 <- ggplot(oly_diag, aes(x=.cooksd)) + geom_histogram() + 
  theme_xkcd() + xkcdaxis(c(0,0.9),c(0,60))
p2 <- ggplot(oly_tf_diag, aes(x=.cooksd)) + geom_histogram() +
  theme_xkcd()+ xkcdaxis(c(0,0.2),c(0,25))
grid.arrange(p1, p2, ncol=2)
```

## Cooks D for simulated data

```{r fig.align='center', fig.width=10, fig.height=4}
p1 <- ggplot(df_diag, aes(x=.cooksd)) + geom_histogram() + 
  ggtitle("All cases") + theme_xkcd() + xkcdaxis(c(0,30),c(0,100))
df_lm <- glm(y~x, data=df[-100,])
df_diag <- augment(df_lm)
p2 <- ggplot(df_diag, aes(x=.cooksd)) + geom_histogram() + ggtitle("Without the outlier") + theme_xkcd() +
  xkcdaxis(c(0,07),c(0,35))
grid.arrange(p1, p2, ncol=2)
```

Values are more spread, when the one extreme value is removed. No other points are influential.

## Solutions

- Remove influential observations, and re-fit model
- Transform explanatory variables to reduce influence
- Use weighted regression to downweight influence of extreme observations

## Your turn

What happens when there are two extreme points with virtually the same values?

## Collinearity

Population and GDP are standardised.

```{r}
olympics2008 <- read_csv("../data/olympics2008.csv")
olympics2012 <- read_csv("../data/olympics2012.csv")

gdp2008 <- read_delim("../data/gdp2008.csv", delim="\t",
                      col_names=FALSE)[,3:4]
colnames(gdp2008) <- c("Country", "GDP")
oly_gdp2008 <- merge(olympics2008, gdp2008, by="Country", all.x=TRUE)

gdp2012 <- read_delim("../data/gdp2012.csv", delim="\t",
                      col_names=FALSE)[,1:2]
colnames(gdp2012) <- c("Country", "GDP")
oly_gdp2012 <- merge(olympics2012, gdp2012, by="Country", all.x=TRUE)

oly_gdp2012 <- merge(oly_gdp2012, olympics2008[,c("Country","Total")],
                     by="Country", all.x=TRUE)
colnames(oly_gdp2012)[7] <- "M2012"
colnames(oly_gdp2012)[13] <- "M2008"
oly_gdp2012[oly_gdp2012$Country=="HongKongChina", "Population"] <- 7155000
oly_gdp2012[is.na(oly_gdp2012$M2008), "M2008"] <- 0
oly_gdp2012 <- oly_gdp2012 %>%
  mutate(Pop_std=scale(Population), GDP_std=scale(GDP))
oly12_lm <- glm(M2012~M2008+Pop_std+GDP_std, data=oly_gdp2012)
oly12_lm_est <- tidy(oly12_lm)$estimate
kable(tidy(oly12_lm))
```

Giving the model $M2012=$ `r oly12_lm_est[1]` $+$ `r oly12_lm_est[2]` $M2008+$ `r oly12_lm_est[3]` $Pop_{std}+$ `r oly12_lm_est[4]` $GDP_{std}+\varepsilon$ 

## Plot the explanatory variables

```{r fig.align='center', fig.width=10, fig.height=5}
library(GGally)
ggscatmat(oly_gdp2012, columns=13:15) 
```

## Explore countries

```{r fig.align='center', fig.width=4, fig.height=4}
p1 <- ggplot(oly_gdp2012, aes(x=M2008, y=Pop_std, label=Country)) + geom_point()
library(plotly)
ggplotly(p1)
```

## Variance inflation factor (VIF)

$$\frac{1}{1-R_j^2}$$

where $R_j^2$ is computed by regressing variable $j$ on all other variables. VIF is a measure the collinearity of the explanatory variables. Values greater than 10 are considered to be high. 

These are the VIFs for the olympic medal tally data:

```{r}
library(car)
vif(oly12_lm)
```

## Suppose we add 2004 counts as an explanatory variable

```{r fig.align='center', fig.width=4, fig.height=4}
olympics2004 <- read_csv("../data/olympics2004.csv")
olympics2004$Code <- substr(olympics2004$Country, 1, 3)
oly_gdp2012 <- merge(oly_gdp2012, olympics2004[,c("Code","Total")],
                     by="Code", all.x=TRUE)
colnames(oly_gdp2012)[16] <- "M2004"
write.csv(oly_gdp2012, file="../data/olympics_gdp_all.csv", 
          quote=FALSE, row.names=FALSE)
ggplot(oly_gdp2012, aes(x=M2004, y=M2008)) + geom_point() + 
  theme_xkcd() + xkcdaxis(c(0,110),c(0,110))
```

## Model

```{r}
oly12_lm <- glm(M2012~M2008+M2004+Pop_std+GDP_std, data=oly_gdp2012)
oly12_lm_est <- tidy(oly12_lm)$estimate
kable(tidy(oly12_lm))
```

Giving the model $M2012=$ `r oly12_lm_est[1]` $+$ `r oly12_lm_est[2]` $M2008+$ `r oly12_lm_est[3]` $M2004+$ `r oly12_lm_est[4]` $Pop_{std}+$ `r oly12_lm_est[5]` $GDP_{std}+\varepsilon$ 

## VIFs

```{r}
vif(oly12_lm)
```

Notice that the VIFs for both 2004 and 2008 are high. 

## Your turn

- Why is it called `Variance Inflation Factor`? Look at the standard deviation of the estimates for the model with 2004 and without 2004.  
- Why would multicollinearity inflate variance of estimates?

## Solutions

- Drop some variables
- Use principal component regression (more advanced courses)
- Partial regression: Fit best variable. Regress next explanatory variable first variable and use the residuals from this fit as the second variable in the model. Continue with other variables. 

## Resources

- [Regression Diagnostics: Identifying Influential Data and Sources of Collinearity](http://onlinelibrary.wiley.com/book/10.1002/0471725153)

## Share and share alike

This work is licensed under the Creative Commons Attribution-Noncommercial 3.0 United States License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/ 3.0/us/ or send a letter to Creative Commons, 171 Second Street, Suite 300, San Francisco, California, 94105, USA.
